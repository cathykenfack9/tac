{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\CATHY\n",
      "[nltk_data]     METANGO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txteconomique/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir une décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1940' #dans notre cas concernent  les fichiers de la periode 1945-1949 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger tous les  fichiers de la décennie et en créer une liste de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KB_JB1051_1945-04-06_01-00001.txt'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de fichiers\n",
    "files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents: 639\n"
     ]
    }
   ],
   "source": [
    "# Nombre de documents\n",
    "nombre_documents = len(texts)\n",
    "\n",
    "# Afficher le nombre de documents\n",
    "print(\"Nombre de documents:\", nombre_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"à servir le pays à l'endroit le plus dangereux EDGARD LALMAND A PARLE A LOUVAIN ROUGE Fondateur t Joseph $4 JACQUEMOTTE Le camarade Lalmand, Ministre de terminer cette guerre rapide- du Ravitaillement, s'est adressé au ment et victorieusement, peuple, au cours d'un grand mee- » Pour mener la guerre, avee suc- ting enthousiaste à 'Louvain. Il a ces, 11 faut intensifier la production brossé un table\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CATHY METANGO\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 85\n",
    "tfidf_array[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CATHY METANGO\\tac\\tac_venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639, 2)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les dimensions de reduced_vectors\n",
    "nombre_documents, nombre_composantes = reduced_vectors.shape\n",
    "\n",
    "print(\"Nombre de documents:\", nombre_documents)\n",
    "print(\"Nombre de composantes principales:\", nombre_composantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANALYSE avec Nuage de mots : nuage de mot pour chaque cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Définir la liste de stopwords\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\"enfin\",\"van\",\"pr\",\"A\",\"F\",\"MN\",\"bon\",\"d'une\",\"k\",\"plat\",\"trè\",\"voir\",\n",
    "       \"ver\",\"but\",\"parti\",\"car\",\"rien\",\"fin\",\"fr\",\"E\",\"A\",\"là\", \"fin\",\"li\", \"lu\", \"ni\", \"c'est\", \"etc\" ,\"sc\",\"déjà\", \"pt\", \"celui\", \"jour\",\"vue\",\n",
    "       \"pay\",\"plusieurs\",\"jeu\", \"part\",  \"ensuite\", \"place\", \"heure\", \"dernier\", \"premier\", \"nouveau\", \"grand\", \"petit\", \"certain\",\n",
    "    \"propre\", \"quelque\", \"chaque\", \"tous\", \"autre\", \"même\", \"très\", \"plus\",\n",
    "    \"moins\", \"assez\", \"bien\", \"peu\", \"trop\", \"aussi\", \"encore\", \"jamais\",\n",
    "    \"toujours\", \"parfois\", \"souvent\", \"rarement\", \"quelquefois\", \"déjà\",\n",
    "    \"encore\", \"presque\", \"simplement\", \"seulement\", \"peut-être\", \"probablement\",\n",
    "    \"vraiment\", \"peut-être\", \"également\", \"ainsi\", \"alors\", \"avant\", \"après\",\n",
    "    \"ici\", \"là\", \"maintenant\", \"hier\", \"aujourd'hui\", \"demain\", \"sous\", \"entre\",\n",
    "    \"sans\", \"contre\", \"vers\" ,  \"aussi\", \"bien\", \"tout\", \"toute\", \"autre\", \"encore\", \"peut-être\", \"seulement\",\n",
    "    \"sans\", \"sous\", \"après\", \"avant\", \"dernier\", \"nouveau\", \"premier\", \"grand\",\n",
    "    \"petit\", \"même\", \"quelque\", \"chaque\", \"tous\", \"plusieurs\", \"certain\", \"divers\",\n",
    "    \"nombreux\", \"propre\", \"quelques\", \"différent\", \"toutefois\", \"ainsi\", \"alors\",\n",
    "    \"avant\", \"très\", \"plus\", \"moins\", \"assez\", \"beaucoup\", \"peu\", \"trop\", \"très\",\n",
    "    \"comme\", \"dont\", \"pendant\", \"vers\", \"contre\", \"entre\", \"sans\", \"chez\" , \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\",\n",
    "        \"donc\", \"cet\", \"sous\", \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\", \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\",\n",
    "          \"doit\", \"contre\", \"depuis\", \"autres\", \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\", \"etc\", \"mod\", \"bel\", \"ruo\", \"adr\", \"ecr\", \"aveo\", \n",
    "          \"app\", \"gar\", \"bx\", \"dos\", \"wai\", \"pet\", \"dés\", \"fer\", \"sal\", \"com\", \"quelques\", \"rien\", \"dès\", \"première\", \"puis\", \"chef\", \"cas\", \"car\", \"cinq\", \n",
    "          \"sujet\", \"pris\", \"hier\", \"fin\", \"elles\", \"effet\", \"declare\", \"ici\", \"voici\", \"celui\", \"trop\", \"doivent\", \"suite\", \"matin\", \"soir\", \"hier\", \"tres\",\n",
    "            \"fin\", \"chez\", \"bon\", \"fois\", \"lieu\", \"quatre\", \"jours\", \"demande\", \"beaucoup\", \"dejà\", \"bureau\", \"paix\", \"tant\", \"maintenat\", \"aucune\", \"grands\", \n",
    "            \"avant\", \"point\", \"grandes\", \"parce\", \"prendre\", \"alors\", \"heure\", \"vient\", \"dejà\", \"afin\", \"vers\", \"mis\", \"mardi\", \"pourrait\", \"services\", \"hui\"\n",
    "              \"souvent\" ,\"peut-être\",\"vraiment\",\"peu\",\"bien\",\"très\",\"beaucoup\",\"tout\",\"toute\",\"toutes\",\"tous\",\"plusieurs\",\"certains\",\"certaine\",\"certaines\",\n",
    "                \"divers\",\"diverse\",\"plus\",\"moins\",\"assez\",\"tellement\",\"hier\",\"aujourd'hui\",\"demain\",\"jamais\",\"souvent\",\"parfois\",\"rarement\",\"quelquefois\",\n",
    "                \"actuellement\",\"déjà\",\"encore\",\"enfin\",\"ainsi\",\"alors\",\"avant\",\"après\",\"ensuite\",\"envers\",\"contre\", \"nouveau\", \"nouvelle\", \"premier\", \n",
    "                \"première\", \"grand\", \"grande\", \"plusieurs\", \"certains\", \"certaines\", \"beaucoup\", \"tout\", \"toute\", \"toutes\", \"tous\",\n",
    "    \"autre\", \"autres\", \"même\", \"peu\", \"très\", \"moins\", \"assez\", \"tellement\", \"plus\", \"moins\", \"ainsi\", \"alors\", \"encore\", \"jamais\", \"toujours\", \"souvent\",\n",
    "      \"parfois\", \"rarement\", \"quelquefois\", \"peut-être\", \"peut\", \"bien\", \"aussi\", \"déjà\", \"encore\", \"seulement\", \"simplement\", \"probablement\", \n",
    "      \"actuellement\",\"particulièrement\", \"notamment\", \"surtout\",\"n'a\", \"qu'elle\", \"quand\", \"s'est\", \"la\", \"ceux\", \"midi\", \"leurs\" ]\n",
    "sw = set(sw)\n",
    "\n",
    "\n",
    "# Vous pourriez ensuite filtrer ces mots de votre analyse textuelle.\n",
    "\n",
    "# Créer un dictionnaire pour stocker les textes dans chaque cluster\n",
    "cluster_texts = {cluster: [] for cluster in range(N_CLUSTERS)}\n",
    "\n",
    "# Ajouter les textes dans chaque cluster\n",
    "for idx, label in enumerate(clusters):\n",
    "    cluster_texts[label].append(texts[idx])\n",
    "\n",
    "# Créer et afficher le nuage de mots pour chaque cluster\n",
    "for cluster_label, cluster_text in cluster_texts.items():\n",
    "    # Concaténer les textes dans le cluster en une seule chaîne\n",
    "    cluster_text_combined = \" \".join(cluster_text)\n",
    "\n",
    "    # Éliminer les stopwords de la chaîne de texte\n",
    "    cluster_text_combined = ' '.join([word for word in cluster_text_combined.split() if word.lower() not in sw])\n",
    "   \n",
    "    # Créer le nuage de mots avec les stopwords filtrés\n",
    "    wordcloud = WordCloud(width=2000, height=1000, background_color=\"white\", stopwords=sw).generate(cluster_text_combined)\n",
    "    \n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title(f\"Cluster {cluster_label} Word Cloud\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f'wordcloudcluster{cluster_label}.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
