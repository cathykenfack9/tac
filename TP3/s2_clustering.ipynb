{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/txt/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir une décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADE = '1960'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger tous les  fichiers de la décennie et en créer une liste de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fichiers\n",
    "files[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de documents\n",
    "nombre_documents = len(texts)\n",
    "\n",
    "# Afficher le nombre de documents\n",
    "print(\"Nombre de documents:\", nombre_documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de textes\n",
    "texts[0][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoriser les documents à l'aide de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle TF-IDF avec ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détail de la matrice\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur tf-IDF du premier document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    tfidf_vectors[0].toarray()[0],\n",
    "    index=vectorizer.get_feature_names_out()\n",
    "    ).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendre les vecteurs et leurs \"distances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine([1, 2, 3], [2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur nos documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 0\n",
    "tfidf_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vecteur du document 85\n",
    "tfidf_array[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(tfidf_array[0], tfidf_array[85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un nombre de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instancier le modèle K-Means et ses arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_model = KMeans(n_clusters=N_CLUSTERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquer le clustering à l'aide de la fonction `fit_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km_model.fit_predict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(clustering))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser les clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les dimensions de reduced_vectors\n",
    "nombre_documents, nombre_composantes = reduced_vectors.shape\n",
    "\n",
    "print(\"Nombre de documents:\", nombre_documents)\n",
    "print(\"Nombre de composantes principales:\", nombre_composantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectors[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Générer le plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=set(clusters), title=\"Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANALYSE avec Nuage de mots : nuage de mot pour chaque cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Définir la liste de stopwords\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\"enfin\",\"van\",\"pr\",\"A\",\"F\",\"MN\",\"bon\",\"d'une\",\"k\",\"plat\",\"trè\",\"voir\",\"ver\",\"but\",\"parti\",\"car\"\n",
    "       ,\"rien\",\"fin\",\"fr\",\"E\",\"A\",\"là\", \"fin\",\"li\", \"lu\", \"ni\", \"c'est\", \"etc\" ,\"sc\",\"déjà\", \"pt\", \"celui\", \"jour\",\"vue\",\"pay\",\"plusieurs\",\"jeu\", \"part\", \"ensuite\", \"place\", \n",
    "       ]\n",
    "sw = set(sw)\n",
    "\n",
    "# Créer un dictionnaire pour stocker les textes dans chaque cluster\n",
    "cluster_texts = {cluster: [] for cluster in range(N_CLUSTERS)}\n",
    "\n",
    "# Ajouter les textes dans chaque cluster\n",
    "for idx, label in enumerate(clusters):\n",
    "    cluster_texts[label].append(texts[idx])\n",
    "\n",
    "# Créer et afficher le nuage de mots pour chaque cluster\n",
    "for cluster_label, cluster_text in cluster_texts.items():\n",
    "    # Concaténer les textes dans le cluster en une seule chaîne\n",
    "    cluster_text_combined = \" \".join(cluster_text)\n",
    "\n",
    "    # Éliminer les stopwords de la chaîne de texte\n",
    "    cluster_text_combined = ' '.join([word for word in cluster_text_combined.split() if word.lower() not in sw])\n",
    "   \n",
    "    # Créer le nuage de mots avec les stopwords filtrés\n",
    "    wordcloud = WordCloud(width=2000, height=1000, background_color=\"white\", stopwords=sw).generate(cluster_text_combined)\n",
    "    \n",
    "    # Afficher le nuage de mots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title(f\"Cluster {cluster_label} Word Cloud\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f'wordcloudcluster{cluster_label}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#liste des mot clés pour chacun de nos clusters : les keywords de chaque clusters sont sauvegardés dans un fichier txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement des données textuelles\n",
    "data_path = \"../data/txt/\"\n",
    "DECADE = '1960'\n",
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_{DECADE[:-1]}\" in f]\n",
    "texts = [open(data_path + f, \"r\", encoding=\"utf-8\").read() for f in files]\n",
    "\n",
    "\n",
    "# Paramètres pour le filtrage\n",
    "MIN_WORD_LENGTH = 3  # Longueur minimale d'un mot\n",
    "MAX_WORD_LENGTH = 10  # Longueur maximale d'un mot\n",
    "POS_TAGS_TO_INCLUDE = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}  # Noms et verbes\n",
    "\n",
    "# Fonction pour filtrer les mots en fonction de la longueur et de la partie du discours\n",
    "def is_informative_word(word, pos):\n",
    "    return MIN_WORD_LENGTH <= len(word) <= MAX_WORD_LENGTH and pos in POS_TAGS_TO_INCLUDE\n",
    "\n",
    "# Pré-traitement et vectorisation\n",
    "def preprocessing(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "custom_stopwords = set(stopwords.words('french')) | {\"1/2\", \"mn\", \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\", \"pr\", \"ag\", \"ecr\", \"1.\", \"3/4\", \"1/4\", \"2\", \"3.\", \"dem\", \"fem\"}\n",
    "\n",
    "# Convertir l'ensemble de stopwords en liste\n",
    "custom_stopwords_list = list(custom_stopwords)\n",
    "\n",
    "# Configuration de TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=custom_stopwords_list,\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tfidf_vectors = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Clustering\n",
    "N_CLUSTERS = 3\n",
    "km_model = KMeans(n_clusters=N_CLUSTERS)\n",
    "clusters = km_model.fit_predict(tfidf_vectors)\n",
    "\n",
    "# Calcul de la moyenne TF-IDF pour chaque cluster\n",
    "cluster_mean = []\n",
    "for i in range(N_CLUSTERS):\n",
    "    cluster_docs = tfidf_vectors[np.where(clusters == i)]\n",
    "    cluster_mean.append(np.mean(cluster_docs, axis=0))\n",
    "\n",
    "# Obtention des noms des caractéristiques (mots) du vectoriseur TF-IDF\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extraction et enregistrement des mots-clés pour chaque cluster\n",
    "for i in range(N_CLUSTERS):\n",
    "    mean_array = np.asarray(cluster_mean[i]).flatten()\n",
    "    sorted_indices = np.argsort(mean_array)[::-1]\n",
    "\n",
    "    keywords = [feature_names[idx] for idx in sorted_indices][:20]  # Top 20 mots-clés\n",
    "\n",
    "    with open(f'keywords_cluster_{i+1}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(keywords))\n",
    "        print(f'Mots-clés pour le cluster {i+1} enregistrés dans keywords_cluster_{i+1}.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
